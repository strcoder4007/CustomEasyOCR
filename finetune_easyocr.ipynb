{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ea6f70",
   "metadata": {
    "papermill": {
     "duration": 0.008705,
     "end_time": "2023-05-08T10:52:01.125069",
     "exception": false,
     "start_time": "2023-05-08T10:52:01.116364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Indroduction\n",
    "\n",
    "The competition is focused on making graphs accessible to people with visual impairments or other disabilities that make it difficult to read and interpret the values displayed in graphs. In this competition, we are given a set of graphs, along with some data points such as x and y coordinates, x and y-axis values, and bounding boxes. However, the values in the graph images are not always clear, making it challenging to extract accurate data from them.\n",
    "\n",
    "\n",
    "To address this challenge, I have fine-tuned an OCR model using EasyOCR. While pre-trained OCR models may not work accurately in all cases, fine-tuning an OCR model on the dataset provided by the competition can help us achieve better results.\n",
    "\n",
    "\n",
    "In this notebook, I will be sharing my approach and implementation of the OCR model, along with the data pre-processing techniques steps used to extract accurate data from the graph images.\n",
    "\n",
    "\n",
    "The goal of this notebook is to provide a detailed guide for anyone interested in using OCR to make graphs accessible to a wider audience. By sharing my approach, I hope to contribute to the development of more effective methods for making visual data accessible to all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384fa9a",
   "metadata": {
    "papermill": {
     "duration": 0.006391,
     "end_time": "2023-05-08T10:52:01.138368",
     "exception": false,
     "start_time": "2023-05-08T10:52:01.131977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d401f1af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:01.153128Z",
     "iopub.status.busy": "2023-05-08T10:52:01.152641Z",
     "iopub.status.idle": "2023-05-08T10:52:01.333998Z",
     "shell.execute_reply": "2023-05-08T10:52:01.332983Z"
    },
    "papermill": {
     "duration": 0.191459,
     "end_time": "2023-05-08T10:52:01.336394",
     "exception": false,
     "start_time": "2023-05-08T10:52:01.144935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2391f1f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:01.351396Z",
     "iopub.status.busy": "2023-05-08T10:52:01.351095Z",
     "iopub.status.idle": "2023-05-08T10:52:13.127685Z",
     "shell.execute_reply": "2023-05-08T10:52:13.126588Z"
    },
    "papermill": {
     "duration": 11.786786,
     "end_time": "2023-05-08T10:52:13.130069",
     "exception": false,
     "start_time": "2023-05-08T10:52:01.343283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyOCR'...\r\n",
      "remote: Enumerating objects: 2551, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\r\n",
      "remote: Total 2551 (delta 2), reused 4 (delta 0), pack-reused 2541\u001b[K\r\n",
      "Receiving objects: 100% (2551/2551), 148.72 MiB | 21.48 MiB/s, done.\r\n",
      "Resolving deltas: 100% (1527/1527), done.\r\n",
      "Updating files: 100% (301/301), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JaidedAI/EasyOCR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac86aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:13.148609Z",
     "iopub.status.busy": "2023-05-08T10:52:13.148289Z",
     "iopub.status.idle": "2023-05-08T10:52:14.093306Z",
     "shell.execute_reply": "2023-05-08T10:52:14.092037Z"
    },
    "papermill": {
     "duration": 0.957147,
     "end_time": "2023-05-08T10:52:14.095747",
     "exception": false,
     "start_time": "2023-05-08T10:52:13.138600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/EasyOCR/trainer /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b62001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:14.114303Z",
     "iopub.status.busy": "2023-05-08T10:52:14.113978Z",
     "iopub.status.idle": "2023-05-08T10:52:25.676343Z",
     "shell.execute_reply": "2023-05-08T10:52:25.675096Z"
    },
    "papermill": {
     "duration": 11.574395,
     "end_time": "2023-05-08T10:52:25.678720",
     "exception": false,
     "start_time": "2023-05-08T10:52:14.104325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = os.listdir(\"/kaggle/working/trainer\")\n",
    "for file in files:\n",
    "    !cp -r /kaggle/working/trainer/{file} /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9c8488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:25.698528Z",
     "iopub.status.busy": "2023-05-08T10:52:25.696954Z",
     "iopub.status.idle": "2023-05-08T10:52:27.636607Z",
     "shell.execute_reply": "2023-05-08T10:52:27.635330Z"
    },
    "papermill": {
     "duration": 1.951473,
     "end_time": "2023-05-08T10:52:27.639001",
     "exception": false,
     "start_time": "2023-05-08T10:52:25.687528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/EasyOCR\n",
    "!rm -r /kaggle/working/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d2f8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.658342Z",
     "iopub.status.busy": "2023-05-08T10:52:27.656836Z",
     "iopub.status.idle": "2023-05-08T10:52:27.662520Z",
     "shell.execute_reply": "2023-05-08T10:52:27.661727Z"
    },
    "papermill": {
     "duration": 0.016922,
     "end_time": "2023-05-08T10:52:27.664308",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.647386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"/kaggle/working/all_data/en_train_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c115dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.682035Z",
     "iopub.status.busy": "2023-05-08T10:52:27.681296Z",
     "iopub.status.idle": "2023-05-08T10:52:27.685199Z",
     "shell.execute_reply": "2023-05-08T10:52:27.684327Z"
    },
    "papermill": {
     "duration": 0.014783,
     "end_time": "2023-05-08T10:52:27.687050",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.672267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# !cp -r /kaggle/input/ocr-dataset/train/train /kaggle/working/all_data\n",
    "# !cp -r /kaggle/input/ocr-dataset/valid/valid /kaggle/working/all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb72e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.704752Z",
     "iopub.status.busy": "2023-05-08T10:52:27.703988Z",
     "iopub.status.idle": "2023-05-08T10:52:27.708349Z",
     "shell.execute_reply": "2023-05-08T10:52:27.707540Z"
    },
    "papermill": {
     "duration": 0.015122,
     "end_time": "2023-05-08T10:52:27.710199",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.695077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = os.listdir(\"/kaggle/input/ocr-dataset/valid/valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfec48fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.727932Z",
     "iopub.status.busy": "2023-05-08T10:52:27.727101Z",
     "iopub.status.idle": "2023-05-08T10:52:27.731239Z",
     "shell.execute_reply": "2023-05-08T10:52:27.730465Z"
    },
    "papermill": {
     "duration": 0.01498,
     "end_time": "2023-05-08T10:52:27.733130",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.718150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.listdir(\"all_data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81909511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.751445Z",
     "iopub.status.busy": "2023-05-08T10:52:27.751191Z",
     "iopub.status.idle": "2023-05-08T10:52:27.755095Z",
     "shell.execute_reply": "2023-05-08T10:52:27.754222Z"
    },
    "papermill": {
     "duration": 0.015105,
     "end_time": "2023-05-08T10:52:27.756849",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.741744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.rename(\"/kaggle/working/all_data/train\",\"/kaggle/working/all_data/en_train_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab579b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.774043Z",
     "iopub.status.busy": "2023-05-08T10:52:27.773513Z",
     "iopub.status.idle": "2023-05-08T10:52:27.777757Z",
     "shell.execute_reply": "2023-05-08T10:52:27.777013Z"
    },
    "papermill": {
     "duration": 0.014866,
     "end_time": "2023-05-08T10:52:27.779599",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.764733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"/kaggle/working/all_data/en_train_filtered/__results___files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b880e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.796936Z",
     "iopub.status.busy": "2023-05-08T10:52:27.796354Z",
     "iopub.status.idle": "2023-05-08T10:52:27.800160Z",
     "shell.execute_reply": "2023-05-08T10:52:27.799304Z"
    },
    "papermill": {
     "duration": 0.014338,
     "end_time": "2023-05-08T10:52:27.801926",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.787588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.listdir(\"/kaggle/input/ocr-dataset/valid/valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec159f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.819287Z",
     "iopub.status.busy": "2023-05-08T10:52:27.818519Z",
     "iopub.status.idle": "2023-05-08T10:52:27.822485Z",
     "shell.execute_reply": "2023-05-08T10:52:27.821734Z"
    },
    "papermill": {
     "duration": 0.014436,
     "end_time": "2023-05-08T10:52:27.824258",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.809822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"labels.csv\" in os.listdir(\"all_data/en_train_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2053946c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.841589Z",
     "iopub.status.busy": "2023-05-08T10:52:27.840857Z",
     "iopub.status.idle": "2023-05-08T10:52:27.844802Z",
     "shell.execute_reply": "2023-05-08T10:52:27.844052Z"
    },
    "papermill": {
     "duration": 0.014489,
     "end_time": "2023-05-08T10:52:27.846603",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.832114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mv /kaggle/working/all_data/en_train_filtered/labels.csv /kaggle/working/all_data/en_train_filtered/__results___files/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2bbf30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.864184Z",
     "iopub.status.busy": "2023-05-08T10:52:27.863457Z",
     "iopub.status.idle": "2023-05-08T10:52:27.867283Z",
     "shell.execute_reply": "2023-05-08T10:52:27.866474Z"
    },
    "papermill": {
     "duration": 0.014563,
     "end_time": "2023-05-08T10:52:27.869082",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.854519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.rename(\"/kaggle/working/all_data/valid\",\"/kaggle/working/all_data/en_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84268cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.886015Z",
     "iopub.status.busy": "2023-05-08T10:52:27.885736Z",
     "iopub.status.idle": "2023-05-08T10:52:27.890087Z",
     "shell.execute_reply": "2023-05-08T10:52:27.889286Z"
    },
    "papermill": {
     "duration": 0.014941,
     "end_time": "2023-05-08T10:52:27.891841",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.876900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"/kaggle/working/all_data/en_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c24969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.909528Z",
     "iopub.status.busy": "2023-05-08T10:52:27.908751Z",
     "iopub.status.idle": "2023-05-08T10:52:27.913055Z",
     "shell.execute_reply": "2023-05-08T10:52:27.912291Z"
    },
    "papermill": {
     "duration": 0.014717,
     "end_time": "2023-05-08T10:52:27.914747",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.900030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"/kaggle/working/all_data/en_val/__results___files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34abc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.932103Z",
     "iopub.status.busy": "2023-05-08T10:52:27.931272Z",
     "iopub.status.idle": "2023-05-08T10:52:27.935379Z",
     "shell.execute_reply": "2023-05-08T10:52:27.934635Z"
    },
    "papermill": {
     "duration": 0.014618,
     "end_time": "2023-05-08T10:52:27.937137",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.922519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mv /kaggle/working/all_data/en_val/labels.csv /kaggle/working/all_data/en_val/__results___files/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cceaec",
   "metadata": {
    "papermill": {
     "duration": 0.007803,
     "end_time": "2023-05-08T10:52:27.952772",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.944969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcdfcabb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:52:27.969923Z",
     "iopub.status.busy": "2023-05-08T10:52:27.969653Z",
     "iopub.status.idle": "2023-05-08T11:11:27.875796Z",
     "shell.execute_reply": "2023-05-08T11:11:27.874475Z"
    },
    "papermill": {
     "duration": 1139.917433,
     "end_time": "2023-05-08T11:11:27.878025",
     "exception": false,
     "start_time": "2023-05-08T10:52:27.960592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter: 0\n",
      "Current Iter: 1000\n",
      "Current Iter: 2000\n",
      "Current Iter: 3000\n",
      "Current Iter: 4000\n",
      "Current Iter: 5000\n",
      "Current Iter: 6000\n",
      "Current Iter: 7000\n",
      "Current Iter: 8000\n",
      "Current Iter: 9000\n",
      "Current Iter: 10000\n",
      "Current Iter: 11000\n",
      "Current Iter: 12000\n",
      "Current Iter: 13000\n",
      "Current Iter: 14000\n",
      "Current Iter: 15000\n",
      "Current Iter: 16000\n",
      "Current Iter: 17000\n",
      "Current Iter: 18000\n",
      "Current Iter: 19000\n",
      "Current Iter: 20000\n",
      "Current Iter: 21000\n",
      "Current Iter: 22000\n",
      "Current Iter: 23000\n",
      "Current Iter: 24000\n",
      "Current Iter: 25000\n",
      "Current Iter: 26000\n",
      "Current Iter: 27000\n",
      "Current Iter: 28000\n",
      "Current Iter: 29000\n",
      "Current Iter: 30000\n",
      "Current Iter: 31000\n",
      "Current Iter: 32000\n",
      "Current Iter: 33000\n",
      "Current Iter: 34000\n",
      "Current Iter: 35000\n",
      "Current Iter: 36000\n",
      "Current Iter: 37000\n",
      "Current Iter: 38000\n",
      "Current Iter: 39000\n",
      "Current Iter: 40000\n",
      "Current Iter: 41000\n",
      "Current Iter: 42000\n",
      "Current Iter: 43000\n",
      "Current Iter: 44000\n",
      "Current Iter: 45000\n",
      "Current Iter: 46000\n",
      "Current Iter: 47000\n",
      "Current Iter: 48000\n",
      "Current Iter: 49000\n",
      "Current Iter: 50000\n",
      "Current Iter: 51000\n",
      "Current Iter: 52000\n",
      "Current Iter: 53000\n",
      "Current Iter: 54000\n",
      "Current Iter: 55000\n",
      "Current Iter: 56000\n",
      "Current Iter: 57000\n",
      "Current Iter: 58000\n",
      "Current Iter: 59000\n",
      "Current Iter: 60000\n",
      "CPU times: user 3min 5s, sys: 57.2 s, total: 4min 2s\n",
      "Wall time: 18min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "BASE_DIR = '/kaggle/input/benetech-making-graphs-accessible'\n",
    "files = os.listdir(f\"{BASE_DIR}/train/images\")\n",
    "f = open(f\"{BASE_DIR}/train/annotations/0000ae6cbdb1.json\")\n",
    "annotated_data = json.load(f)\n",
    "# annotated_data['text'][10]annotated_data['text'][10]\n",
    "def hwlt2ltrb(coor):\n",
    "    left = coor[2]\n",
    "    top = coor[3]\n",
    "    right = left + coor[1]\n",
    "    bottom = top + coor[0]\n",
    "    \n",
    "    return (left,top,right,bottom)\n",
    "\n",
    "\n",
    "def get_ltrb(source,idx):\n",
    "    l,t,r,b = float(\"inf\"),float(\"inf\"),float(\"-inf\"),float(\"-inf\")\n",
    "    data = source[idx][\"polygon\"]\n",
    "    x_values = (data['x0'],data['x1'],data['x2'],data['x3'])\n",
    "    y_values = (data['y0'],data['y1'],data['y2'],data['y3'])\n",
    "    l = min(l,min(x_values))\n",
    "    t = min(t,min(y_values))\n",
    "    r = max(r,max(x_values))\n",
    "    b = max(b,max(y_values))\n",
    "    text = source[idx]['text']\n",
    "    return (l,t,r,b),text\n",
    "# !mkdir ./output/train\n",
    "# !mkdir ./output/valid\n",
    "# mode = 'train'\n",
    "\n",
    "classes = {\n",
    "    'line': 0,\n",
    "    'scatter':1,\n",
    "    'dot':2,\n",
    "    'vertical_bar':3,\n",
    "    'horizontal_bar':4,\n",
    "    'X-axis':5,\n",
    "    'y-axis':6\n",
    "}\n",
    "\n",
    "train_image_names = []\n",
    "train_texts = []\n",
    "\n",
    "valid_image_names = []\n",
    "valid_texts = []\n",
    "\n",
    "mode = \"train\"\n",
    "\n",
    "for i,file in enumerate(files):\n",
    "    img = cv2.imread(f\"{BASE_DIR}/train/images/{file}\")\n",
    "    image = img.copy()\n",
    "    h,w,_ = img.shape\n",
    "    f = open(f\"{BASE_DIR}/train/annotations/{file.replace('jpg','json')}\")\n",
    "    annotated_data = json.load(f)\n",
    "    \n",
    "    indicies = list(map(lambda x: x[\"id\"], annotated_data[\"axes\"][\"x-axis\"][\"ticks\"]))\n",
    "    \n",
    "    indicies.extend(list(map(lambda x: x[\"id\"], annotated_data[\"axes\"][\"y-axis\"][\"ticks\"])))\n",
    "    \n",
    "#     plot_l,plot_t,plot_r,plot_b = hwlt2ltrb((plot_bb[\"height\"],plot_bb[\"width\"],plot_bb[\"x0\"],plot_bb[\"y0\"]))\n",
    "#     try:\n",
    "    for idx in indicies:\n",
    "        try:\n",
    "            coor,text = get_ltrb(annotated_data['text'],idx)\n",
    "            l,t,r,b = coor\n",
    "            canvas = cv2.rectangle(image,(l,t),(r,b),(0,255,0),1)\n",
    "            text_crop = img[t:b,l:r]\n",
    "            \n",
    "            text_img_name = f\"{file.replace('.jpg','')}_{idx}.jpg\"\n",
    "            if mode == \"train\":\n",
    "                cv2.imwrite(f\"/kaggle/working/all_data/en_train_filtered/__results___files/{text_img_name}\",text_crop)\n",
    "                train_image_names.append(text_img_name)\n",
    "                train_texts.append(text)\n",
    "            else:\n",
    "                cv2.imwrite(f\"/kaggle/working/all_data/en_val/__results___files/{text_img_name}\",text_crop) \n",
    "                valid_image_names.append(text_img_name)\n",
    "                valid_texts.append(text)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    if i == 50000:\n",
    "        mode = \"valid\"\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Current Iter: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fd209a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:27.902886Z",
     "iopub.status.busy": "2023-05-08T11:11:27.902561Z",
     "iopub.status.idle": "2023-05-08T11:11:29.979827Z",
     "shell.execute_reply": "2023-05-08T11:11:29.978899Z"
    },
    "papermill": {
     "duration": 2.092341,
     "end_time": "2023-05-08T11:11:29.982085",
     "exception": false,
     "start_time": "2023-05-08T11:11:27.889744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(list(zip(train_image_names, train_texts)),\n",
    "               columns =['filename', 'words'])\n",
    "\n",
    "valid_df = pd.DataFrame(list(zip(valid_image_names, valid_texts)),\n",
    "               columns =['filename', 'words'])\n",
    "train_df.to_csv('/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv',index=False)\n",
    "valid_df.to_csv('/kaggle/working/all_data/en_val/__results___files/labels.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e87895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:30.005842Z",
     "iopub.status.busy": "2023-05-08T11:11:30.005518Z",
     "iopub.status.idle": "2023-05-08T11:11:36.640210Z",
     "shell.execute_reply": "2023-05-08T11:11:36.639012Z"
    },
    "papermill": {
     "duration": 6.649407,
     "end_time": "2023-05-08T11:11:36.642853",
     "exception": false,
     "start_time": "2023-05-08T11:11:29.993446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\", sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "train_df = train_df.dropna().reset_index(drop=True)\n",
    "train_df.to_csv(\"/kaggle/working/all_data/en_train_filtered/__results___files/labels.csv\",index=False)\n",
    "\n",
    "valid_df = pd.read_csv(\"/kaggle/working/all_data/en_val/__results___files/labels.csv\", sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "valid_df = valid_df.dropna().reset_index(drop=True)\n",
    "valid_df.to_csv(\"/kaggle/working/all_data/en_val/__results___files/labels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af49019f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:36.668816Z",
     "iopub.status.busy": "2023-05-08T11:11:36.668207Z",
     "iopub.status.idle": "2023-05-08T11:11:36.675812Z",
     "shell.execute_reply": "2023-05-08T11:11:36.674820Z"
    },
    "papermill": {
     "duration": 0.022796,
     "end_time": "2023-05-08T11:11:36.677920",
     "exception": false,
     "start_time": "2023-05-08T11:11:36.655124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_train_filtered', 'folder.txt', 'en_val']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4a728f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:36.701395Z",
     "iopub.status.busy": "2023-05-08T11:11:36.701111Z",
     "iopub.status.idle": "2023-05-08T11:11:36.705064Z",
     "shell.execute_reply": "2023-05-08T11:11:36.704159Z"
    },
    "papermill": {
     "duration": 0.017624,
     "end_time": "2023-05-08T11:11:36.706836",
     "exception": false,
     "start_time": "2023-05-08T11:11:36.689212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa3b3116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:36.731975Z",
     "iopub.status.busy": "2023-05-08T11:11:36.731446Z",
     "iopub.status.idle": "2023-05-08T11:11:36.735541Z",
     "shell.execute_reply": "2023-05-08T11:11:36.734641Z"
    },
    "papermill": {
     "duration": 0.019612,
     "end_time": "2023-05-08T11:11:36.737417",
     "exception": false,
     "start_time": "2023-05-08T11:11:36.717805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba35737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:36.760842Z",
     "iopub.status.busy": "2023-05-08T11:11:36.760135Z",
     "iopub.status.idle": "2023-05-08T11:11:50.711374Z",
     "shell.execute_reply": "2023-05-08T11:11:50.710333Z"
    },
    "papermill": {
     "duration": 13.965483,
     "end_time": "2023-05-08T11:11:50.713895",
     "exception": false,
     "start_time": "2023-05-08T11:11:36.748412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\r\n",
      "  Downloading natsort-8.3.1-py3-none-any.whl (38 kB)\r\n",
      "Installing collected packages: natsort\r\n",
      "Successfully installed natsort-8.3.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install natsort\n",
    "\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "from utils import AttrDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d56ed687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:50.740075Z",
     "iopub.status.busy": "2023-05-08T11:11:50.738322Z",
     "iopub.status.idle": "2023-05-08T11:11:50.743560Z",
     "shell.execute_reply": "2023-05-08T11:11:50.742724Z"
    },
    "papermill": {
     "duration": 0.019457,
     "end_time": "2023-05-08T11:11:50.745369",
     "exception": false,
     "start_time": "2023-05-08T11:11:50.725912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaddf1d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:50.769077Z",
     "iopub.status.busy": "2023-05-08T11:11:50.768773Z",
     "iopub.status.idle": "2023-05-08T11:11:50.775353Z",
     "shell.execute_reply": "2023-05-08T11:11:50.774476Z"
    },
    "papermill": {
     "duration": 0.020551,
     "end_time": "2023-05-08T11:11:50.777246",
     "exception": false,
     "start_time": "2023-05-08T11:11:50.756695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa20c089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:50.800940Z",
     "iopub.status.busy": "2023-05-08T11:11:50.800656Z",
     "iopub.status.idle": "2023-05-08T11:11:50.807135Z",
     "shell.execute_reply": "2023-05-08T11:11:50.806188Z"
    },
    "papermill": {
     "duration": 0.021631,
     "end_time": "2023-05-08T11:11:50.810125",
     "exception": false,
     "start_time": "2023-05-08T11:11:50.788494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config_files/en_filtered_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config_files/en_filtered_config.yaml\n",
    "number: '0123456789'\n",
    "symbol: \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ â‚¬\"\n",
    "lang_char: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "experiment_name: 'en_filtered'\n",
    "train_data: 'all_data'\n",
    "valid_data: 'all_data/en_val'\n",
    "manualSeed: 1111\n",
    "workers: 2\n",
    "batch_size: 16 # 32\n",
    "num_iter: 3000 \n",
    "valInterval: 1000\n",
    "saved_model: '' #'saved_models/en_filtered/iter_300000.pth'\n",
    "FT: False\n",
    "optim: False # default is Adadelta\n",
    "lr: 1.\n",
    "beta1: 0.9\n",
    "rho: 0.95\n",
    "eps: 0.00000001\n",
    "grad_clip: 5\n",
    "#Data processing\n",
    "select_data: 'en_train_filtered' # this is dataset folder in train_data\n",
    "batch_ratio: '1' \n",
    "total_data_usage_ratio: 1.0\n",
    "batch_max_length: 34 \n",
    "imgH: 64\n",
    "imgW: 600\n",
    "rgb: False\n",
    "contrast_adjust: False\n",
    "sensitive: True\n",
    "PAD: True\n",
    "contrast_adjust: 0.0\n",
    "data_filtering_off: False\n",
    "# Model Architecture\n",
    "Transformation: 'None'\n",
    "FeatureExtraction: 'VGG'\n",
    "SequenceModeling: 'BiLSTM'\n",
    "Prediction: 'CTC'\n",
    "num_fiducial: 20\n",
    "input_channel: 1\n",
    "output_channel: 256\n",
    "hidden_size: 256\n",
    "decode: 'greedy'\n",
    "new_prediction: False\n",
    "freeze_FeatureFxtraction: False\n",
    "freeze_SequenceModeling: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9770ed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:50.834073Z",
     "iopub.status.busy": "2023-05-08T11:11:50.833714Z",
     "iopub.status.idle": "2023-05-08T11:11:50.843590Z",
     "shell.execute_reply": "2023-05-08T11:11:50.842816Z"
    },
    "papermill": {
     "duration": 0.023904,
     "end_time": "2023-05-08T11:11:50.845317",
     "exception": false,
     "start_time": "2023-05-08T11:11:50.821413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = get_config(\"config_files/en_filtered_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6e7e30c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:50.869626Z",
     "iopub.status.busy": "2023-05-08T11:11:50.869376Z",
     "iopub.status.idle": "2023-05-08T11:11:50.873097Z",
     "shell.execute_reply": "2023-05-08T11:11:50.872175Z"
    },
    "papermill": {
     "duration": 0.018153,
     "end_time": "2023-05-08T11:11:50.874844",
     "exception": false,
     "start_time": "2023-05-08T11:11:50.856691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = os.listdir('/kaggle/working/all_data/en_train_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "869b09b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:50.899295Z",
     "iopub.status.busy": "2023-05-08T11:11:50.898614Z",
     "iopub.status.idle": "2023-05-08T11:11:51.165105Z",
     "shell.execute_reply": "2023-05-08T11:11:51.164143Z"
    },
    "papermill": {
     "duration": 0.281272,
     "end_time": "2023-05-08T11:11:51.167402",
     "exception": false,
     "start_time": "2023-05-08T11:11:50.886130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile /kaggle/working/dataset.py\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torch\n",
    "import pandas  as pd\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "from torch._utils import _accumulate\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def contrast_grey(img):\n",
    "    high = np.percentile(img, 90)\n",
    "    low  = np.percentile(img, 10)\n",
    "    return (high-low)/(high+low), high, low\n",
    "\n",
    "def adjust_contrast_grey(img, target = 0.4):\n",
    "    contrast, high, low = contrast_grey(img)\n",
    "    if contrast < target:\n",
    "        img = img.astype(int)\n",
    "        ratio = 200./(high-low)\n",
    "        img = (img - low + 25)*ratio\n",
    "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image,text = next(iter(data_loader_iter))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = next(iter(self.dataloader_iter_list[i]))\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts\n",
    "\n",
    "\n",
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = OCRDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        print(root)\n",
    "        self.df = pd.read_csv(os.path.join(root,'labels.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "        self.nSamples = len(self.df)\n",
    "\n",
    "        if self.opt.data_filtering_off:\n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "        else:\n",
    "            self.filtered_index_list = []\n",
    "            for index in range(self.nSamples):\n",
    "                label = self.df.at[index,'words']\n",
    "                try:\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(label)\n",
    "                out_of_char = f'[^{self.opt.character}]'\n",
    "                if re.search(out_of_char, label.lower()):\n",
    "                    continue\n",
    "                self.filtered_index_list.append(index)\n",
    "            self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.filtered_index_list[index]\n",
    "        img_fname = self.df.at[index,'filename']\n",
    "        img_fpath = os.path.join(self.root, img_fname)\n",
    "        label = self.df.at[index,'words']\n",
    "\n",
    "        if self.opt.rgb:\n",
    "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
    "        else:\n",
    "            img = Image.open(img_fpath).convert('L')\n",
    "\n",
    "        if not self.opt.sensitive:\n",
    "            label = label.lower()\n",
    "\n",
    "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "        out_of_char = f'[^{self.opt.character}]'\n",
    "        label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.contrast_adjust = contrast_adjust\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "\n",
    "                #### augmentation here - change contrast\n",
    "                if self.contrast_adjust > 0:\n",
    "                    image = np.array(image.convert(\"L\"))\n",
    "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
    "                    image = Image.fromarray(image, 'L')\n",
    "\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    image_numpy = image_tensor.cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e157b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:51.193099Z",
     "iopub.status.busy": "2023-05-08T11:11:51.192751Z",
     "iopub.status.idle": "2023-05-08T11:11:52.718354Z",
     "shell.execute_reply": "2023-05-08T11:11:52.717355Z"
    },
    "papermill": {
     "duration": 1.541605,
     "end_time": "2023-05-08T11:11:52.721269",
     "exception": false,
     "start_time": "2023-05-08T11:11:51.179664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /kaggle/working/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "from utils import CTCLabelConverter, AttnLabelConverter, Averager\n",
    "# from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from model import Model\n",
    "from test import validation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    print(\"Modules, Parameters\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        #table.add_row([name, param])\n",
    "        total_params+=param\n",
    "        print(name, param)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "def train(opt, show_number = 2, amp=False):\n",
    "    \"\"\" dataset preparation \"\"\"\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "    \n",
    "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(opt.workers), prefetch_factor=512,\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "    log.write(valid_dataset_log)\n",
    "    print('-' * 80)\n",
    "    log.write('-' * 80 + '\\n')\n",
    "    log.close()\n",
    "    \n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        pretrained_dict = torch.load(opt.saved_model)\n",
    "        if opt.new_prediction:\n",
    "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n",
    "        \n",
    "        model = torch.nn.DataParallel(model).to(device) \n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(pretrained_dict, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(pretrained_dict)\n",
    "        if opt.new_prediction:\n",
    "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n",
    "            for name, param in model.module.Prediction.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            model = model.to(device) \n",
    "    else:\n",
    "        # weight initialization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'localization_fc2' in name:\n",
    "                print(f'Skip {name} as it is already initialized')\n",
    "                continue\n",
    "            try:\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            except Exception as e:  # for batchnorm.\n",
    "                if 'weight' in name:\n",
    "                    param.data.fill_(1)\n",
    "                continue\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    \n",
    "    model.train() \n",
    "    print(\"Model:\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    \"\"\" setup loss \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
    "    # loss averager\n",
    "    loss_avg = Averager()\n",
    "\n",
    "    # freeze some layers\n",
    "    try:\n",
    "        if opt.freeze_FeatureFxtraction:\n",
    "            for param in model.module.FeatureExtraction.parameters():\n",
    "                param.requires_grad = False\n",
    "        if opt.freeze_SequenceModeling:\n",
    "            for param in model.module.SequenceModeling.parameters():\n",
    "                param.requires_grad = False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # filter that only require gradient decent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params num : ', sum(params_num))\n",
    "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
    "\n",
    "    # setup optimizer\n",
    "    if opt.optim=='adam':\n",
    "        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        optimizer = optim.Adam(filtered_parameters)\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "    print(\"Optimizer:\")\n",
    "    print(optimizer)\n",
    "\n",
    "    \"\"\" final options \"\"\"\n",
    "    # print(opt)\n",
    "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n",
    "        opt_log = '------------ Options -------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "        opt_log += '---------------------------------------\\n'\n",
    "        print(opt_log)\n",
    "        opt_file.write(opt_log)\n",
    "\n",
    "    \"\"\" start training \"\"\"\n",
    "    start_iter = 0\n",
    "    if opt.saved_model != '':\n",
    "        try:\n",
    "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "            print(f'continue to train, start_iter: {start_iter}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    i = start_iter\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    t1= time.time()\n",
    "        \n",
    "    while(True):\n",
    "        # train part\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if amp:\n",
    "            with autocast():\n",
    "                image_tensors, labels = train_dataset.get_batch()\n",
    "                image = image_tensors.to(device)\n",
    "                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "                batch_size = image.size(0)\n",
    "\n",
    "                if 'CTC' in opt.Prediction:\n",
    "                    preds = model(image, text).log_softmax(2)\n",
    "                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                    preds = preds.permute(1, 0, 2)\n",
    "                    torch.backends.cudnn.enabled = False\n",
    "                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                    torch.backends.cudnn.enabled = True\n",
    "                else:\n",
    "                    preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                    target = text[:, 1:]  # without [GO] Symbol\n",
    "                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            scaler.scale(cost).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            image_tensors, labels = train_dataset.get_batch()\n",
    "            image = image_tensors.to(device)\n",
    "            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "            batch_size = image.size(0)\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text).log_softmax(2)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                preds = preds.permute(1, 0, 2)\n",
    "                torch.backends.cudnn.enabled = False\n",
    "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                torch.backends.cudnn.enabled = True\n",
    "            else:\n",
    "                preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                target = text[:, 1:]  # without [GO] Symbol\n",
    "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            cost.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n",
    "            optimizer.step()\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (i % opt.valInterval == 0) and (i!=0):\n",
    "            print('training time: ', time.time()-t1)\n",
    "            t1=time.time()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # for log\n",
    "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n",
    "                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                model.train()\n",
    "\n",
    "                # training loss and validation loss\n",
    "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
    "\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'/kaggle/working/best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'/kaggle/working/best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                \n",
    "                #show_number = min(show_number, len(labels))\n",
    "                \n",
    "                start = random.randint(0,len(labels) - show_number )    \n",
    "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "                print('validation time: ', time.time()-t1)\n",
    "                t1=time.time()\n",
    "        # save model per 1e+4 iter.\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
    "        if i == opt.num_iter:\n",
    "            print('end the training')\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2365307",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T11:11:52.747852Z",
     "iopub.status.busy": "2023-05-08T11:11:52.747530Z",
     "iopub.status.idle": "2023-05-08T12:13:03.650625Z",
     "shell.execute_reply": "2023-05-08T12:13:03.649431Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 3670.931659,
     "end_time": "2023-05-08T12:13:03.666278",
     "exception": false,
     "start_time": "2023-05-08T11:11:52.734619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['en_train_filtered']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: en_train_filtered\n",
      "all_data/en_train_filtered/__results___files\n",
      "sub-directory:\t/en_train_filtered/__results___files\t num samples: 969884\n",
      "num total samples of en_train_filtered: 969884 x 1.0 (total_data_usage_ratio) = 969884\n",
      "num samples of en_train_filtered per batch: 16 x 1.0 (batch_ratio) = 16\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 16 = 16\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data/en_val\t dataset: /\n",
      "all_data/en_val/__results___files\n",
      "sub-directory:\t/__results___files\t num samples: 205557\n",
      "--------------------------------------------------------------------------------\n",
      "No Transformation module specified\n",
      "model input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\n",
      "Model:\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (FeatureExtraction): VGG_FeatureExtractor(\n",
      "      (ConvNet): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU(inplace=True)\n",
      "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): ReLU(inplace=True)\n",
      "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (16): ReLU(inplace=True)\n",
      "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "        (19): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "    (SequenceModeling): Sequential(\n",
      "      (0): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n",
      "  )\n",
      ")\n",
      "Modules, Parameters\n",
      "module.FeatureExtraction.ConvNet.0.weight 288\n",
      "module.FeatureExtraction.ConvNet.0.bias 32\n",
      "module.FeatureExtraction.ConvNet.3.weight 18432\n",
      "module.FeatureExtraction.ConvNet.3.bias 64\n",
      "module.FeatureExtraction.ConvNet.6.weight 73728\n",
      "module.FeatureExtraction.ConvNet.6.bias 128\n",
      "module.FeatureExtraction.ConvNet.8.weight 147456\n",
      "module.FeatureExtraction.ConvNet.8.bias 128\n",
      "module.FeatureExtraction.ConvNet.11.weight 294912\n",
      "module.FeatureExtraction.ConvNet.12.weight 256\n",
      "module.FeatureExtraction.ConvNet.12.bias 256\n",
      "module.FeatureExtraction.ConvNet.14.weight 589824\n",
      "module.FeatureExtraction.ConvNet.15.weight 256\n",
      "module.FeatureExtraction.ConvNet.15.bias 256\n",
      "module.FeatureExtraction.ConvNet.18.weight 262144\n",
      "module.FeatureExtraction.ConvNet.18.bias 256\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.0.linear.weight 131072\n",
      "module.SequenceModeling.0.linear.bias 256\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.1.linear.weight 131072\n",
      "module.SequenceModeling.1.linear.bias 256\n",
      "module.Prediction.weight 24832\n",
      "module.Prediction.bias 97\n",
      "Total Trainable Params: 3781345\n",
      "Trainable params num :  3781345\n",
      "Optimizer:\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1.0\n",
      "    maximize: False\n",
      "    rho: 0.95\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "number: 0123456789\n",
      "symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ â‚¬\n",
      "lang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "experiment_name: en_filtered\n",
      "train_data: all_data\n",
      "valid_data: all_data/en_val\n",
      "manualSeed: 1111\n",
      "workers: 2\n",
      "batch_size: 16\n",
      "num_iter: 3000\n",
      "valInterval: 1000\n",
      "saved_model: \n",
      "FT: False\n",
      "optim: False\n",
      "lr: 1.0\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "select_data: ['en_train_filtered']\n",
      "batch_ratio: ['1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 34\n",
      "imgH: 64\n",
      "imgW: 600\n",
      "rgb: False\n",
      "contrast_adjust: 0.0\n",
      "sensitive: True\n",
      "PAD: True\n",
      "data_filtering_off: False\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 256\n",
      "hidden_size: 256\n",
      "decode: greedy\n",
      "new_prediction: False\n",
      "freeze_FeatureFxtraction: False\n",
      "freeze_SequenceModeling: False\n",
      "character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ â‚¬ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "num_class: 97\n",
      "---------------------------------------\n",
      "\n",
      "training time:  68.50897407531738\n",
      "[1000/3000] Train loss: 2.48758, Valid loss: 1.22334, Elapsed_time: 68.50920\n",
      "Current_accuracy : 61.082, Current_norm_ED  : 0.6982\n",
      "Best_accuracy    : 61.082, Best_norm_ED     : 0.6982\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "1995                      | 1995                      | 0.8407\tTrue\n",
      "Lee                       | Aa                        | 0.0203\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  1151.8297622203827\n",
      "training time:  59.070152282714844\n",
      "[2000/3000] Train loss: 1.00587, Valid loss: 0.90788, Elapsed_time: 1279.40922\n",
      "Current_accuracy : 66.690, Current_norm_ED  : 0.7608\n",
      "Best_accuracy    : 66.690, Best_norm_ED     : 0.7608\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "50                        | 50                        | 0.9273\tTrue\n",
      "30                        | 30                        | 0.8762\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  1160.7607145309448\n",
      "training time:  59.33499312400818\n",
      "[3000/3000] Train loss: 0.88154, Valid loss: 0.80476, Elapsed_time: 2499.50506\n",
      "Current_accuracy : 68.166, Current_norm_ED  : 0.7849\n",
      "Best_accuracy    : 68.166, Best_norm_ED     : 0.7849\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "1000000                   | 1000000                   | 0.5265\tTrue\n",
      "51.7                      | 51.7                      | 0.2498\tTrue\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  1154.7913229465485\n",
      "end the training\n"
     ]
    }
   ],
   "source": [
    "train(opt, amp=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4876.035627,
   "end_time": "2023-05-08T12:13:07.856916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-08T10:51:51.821289",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
